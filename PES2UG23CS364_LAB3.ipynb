{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQ4I2InlxFIueeb00oK7u/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z75XWBdz5YnR","executionInfo":{"status":"ok","timestamp":1755868433454,"user_tz":-330,"elapsed":145,"user":{"displayName":"NANDANI PESU-EC-B.Tech 2023","userId":"10567956852641982349"}},"outputId":"c838e038-4513-4446-cf33-02a646aac3e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ðŸ“‚ Dataset: /content/mushrooms.csv\n","Target column: class\n","Accuracy: 0.3114 (31.14%)\n","Precision (weighted): 0.4558 | Recall: 0.3114 | F1: 0.3666\n","Precision (macro): 0.1303 | Recall: 0.0890 | F1: 0.1048\n","Tree Depth: 0 | Nodes: 0 | Leaves: 0\n","\n","\n","ðŸ“‚ Dataset: /content/tictactoe.csv\n","Target column: Class\n","Accuracy: 0.2865 (28.65%)\n","Precision (weighted): 0.5250 | Recall: 0.2865 | F1: 0.3705\n","Precision (macro): 0.3236 | Recall: 0.1792 | F1: 0.2305\n","Tree Depth: 0 | Nodes: 0 | Leaves: 0\n","\n","\n","ðŸ“‚ Dataset: /content/Nursery.csv\n","Target column: class\n","Accuracy: 0.4811 (48.11%)\n","Precision (weighted): 0.4854 | Recall: 0.4811 | F1: 0.4830\n","Precision (macro): 0.2895 | Recall: 0.4865 | F1: 0.2883\n","Tree Depth: 0 | Nodes: 0 | Leaves: 0\n","\n","\n","ðŸ“Š SUMMARY TABLE\n","      Dataset  Accuracy  Precision_w  Recall_w   F1_w  Depth  Nodes\n","mushrooms.csv    0.3114       0.4558    0.3114 0.3666      0      0\n","tictactoe.csv    0.2865       0.5250    0.2865 0.3705      0      0\n","  Nursery.csv    0.4811       0.4854    0.4811 0.4830      0      0\n"]}],"source":["import sys\n","import importlib\n","import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from collections import Counter\n","\n","# ----------------- Module Import -----------------\n","subname = \"EC_F_PES2UG23CS364_NANDANI_lab3\"  # your implementation file name (without .py)\n","framework = \"pytorch\"                 # pytorch or sklearn\n","\n","try:\n","    mymodule = importlib.import_module(subname)\n","except Exception as e:\n","    print(f\"âŒ Error importing module '{subname}': {e}\")\n","    sys.exit()\n","\n","# Required functions from your module\n","get_selected_attribute = mymodule.get_selected_attribute\n","get_information_gain = mymodule.get_information_gain\n","get_avg_info_of_attribute = mymodule.get_avg_info_of_attribute\n","get_entropy_of_dataset = mymodule.get_entropy_of_dataset\n","\n","# ----------------- Helper Functions -----------------\n","def calculate_accuracy(y_true, y_pred):\n","    if isinstance(y_true, torch.Tensor): y_true = y_true.numpy()\n","    if isinstance(y_pred, torch.Tensor): y_pred = y_pred.numpy()\n","    valid_mask = np.array([p is not None for p in y_pred])\n","    y_true, y_pred = y_true[valid_mask], np.array(y_pred)[valid_mask]\n","    return np.sum(y_true == y_pred) / len(y_true) if len(y_true) > 0 else 0.0\n","\n","def calculate_precision_recall_f1(y_true, y_pred, average='weighted'):\n","    if isinstance(y_true, torch.Tensor): y_true = y_true.numpy()\n","    if isinstance(y_pred, torch.Tensor): y_pred = y_pred.numpy()\n","    valid_mask = np.array([p is not None for p in y_pred])\n","    y_true, y_pred = y_true[valid_mask], np.array(y_pred)[valid_mask]\n","    classes = np.unique(np.concatenate([y_true, y_pred]))\n","    precisions, recalls, f1s, supports = [], [], [], []\n","    for cls in classes:\n","        tp = np.sum((y_true == cls) & (y_pred == cls))\n","        fp = np.sum((y_true != cls) & (y_pred == cls))\n","        fn = np.sum((y_true == cls) & (y_pred != cls))\n","        precision = tp/(tp+fp) if (tp+fp)>0 else 0\n","        recall = tp/(tp+fn) if (tp+fn)>0 else 0\n","        f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0\n","        precisions.append(precision); recalls.append(recall); f1s.append(f1); supports.append(np.sum(y_true==cls))\n","    if average=='weighted':\n","        total = sum(supports)\n","        return (sum(p*s for p,s in zip(precisions,supports))/total,\n","                sum(r*s for r,s in zip(recalls,supports))/total,\n","                sum(f*s for f,s in zip(f1s,supports))/total)\n","    else:\n","        return np.mean(precisions), np.mean(recalls), np.mean(f1s)\n","\n","def preprocess_data(df):\n","    df_processed=df.copy(); label_encoders={}\n","    for col in df_processed.columns:\n","        le=LabelEncoder(); df_processed[col]=le.fit_transform(df[col]); label_encoders[col]=le\n","    return df_processed,label_encoders\n","\n","# ----------------- Prediction Functions (dummy) -----------------\n","def predict_single_sample(tree, sample, cols):\n","    # Since we have no tree, predict most common class in last column\n","    return int(sample[-1])\n","\n","def predict_batch(tree, data, cols):\n","    return [predict_single_sample(tree, sample, cols) for sample in data]\n","\n","def calculate_tree_complexity_metrics(tree):\n","    # No tree => return dummy metrics\n","    return {'max_depth':0,'num_nodes':0,'num_leaves':0,'num_internal_nodes':0}\n","\n","# ----------------- Test Function -----------------\n","def test_case(data_path):\n","    df=pd.read_csv(data_path)\n","    print(f\"\\nðŸ“‚ Dataset: {data_path}\")\n","    print(f\"Target column: {df.columns[-1]}\")\n","    df_processed,_=preprocess_data(df)\n","    dataset=torch.tensor(df_processed.values,dtype=torch.float32)\n","    cols=list(df_processed.columns)\n","\n","    total=len(dataset); train_size=int(0.8*total)\n","    torch.manual_seed(42); idx=torch.randperm(total); data_shuffled=dataset[idx]\n","    train_data,test_data=data_shuffled[:train_size],data_shuffled[train_size:]\n","\n","    # No tree construction\n","    tree = None\n","\n","    # Evaluate\n","    X_test,y_test=test_data[:,:-1],test_data[:,-1]\n","    preds=predict_batch(tree,X_test,cols)\n","    acc=calculate_accuracy(y_test,preds)\n","    p_w,r_w,f1_w=calculate_precision_recall_f1(y_test,preds,average='weighted')\n","    p_m,r_m,f1_m=calculate_precision_recall_f1(y_test,preds,average='macro')\n","    comp=calculate_tree_complexity_metrics(tree)\n","\n","    print(f\"Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n","    print(f\"Precision (weighted): {p_w:.4f} | Recall: {r_w:.4f} | F1: {f1_w:.4f}\")\n","    print(f\"Precision (macro): {p_m:.4f} | Recall: {r_m:.4f} | F1: {f1_m:.4f}\")\n","    print(f\"Tree Depth: {comp['max_depth']} | Nodes: {comp['num_nodes']} | Leaves: {comp['num_leaves']}\\n\")\n","\n","    return {\n","        \"Dataset\": data_path.split(\"/\")[-1],\n","        \"Accuracy\": round(acc,4),\n","        \"Precision_w\": round(p_w,4),\n","        \"Recall_w\": round(r_w,4),\n","        \"F1_w\": round(f1_w,4),\n","        \"Depth\": comp['max_depth'],\n","        \"Nodes\": comp['num_nodes']\n","    }\n","\n","# ----------------- Main -----------------\n","if __name__==\"__main__\":\n","    datasets=[\"/content/mushrooms.csv\",\"/content/tictactoe.csv\",\"/content/Nursery.csv\"]\n","    results=[]\n","    for d in datasets: results.append(test_case(d))\n","    print(\"\\nðŸ“Š SUMMARY TABLE\")\n","    print(pd.DataFrame(results).to_string(index=False))"]}]}